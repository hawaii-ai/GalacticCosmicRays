{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ve8yVrLbiOXv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 15:36:05.431674: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-29 15:36:05.808605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 15:36:07.528658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Retraining NN with Elegy then do HMCon Mana\n",
    "# Author: Peter Nov 5 2022\n",
    "# Edited by Linnea August/September 2023\n",
    "\n",
    "# New Requirements:\n",
    "# conda install python=3.9 numpy scipy pandas matplotlib\n",
    "# conda install -c anaconda cudatoolkit\n",
    "# pip install tensorflow\n",
    "# pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\n",
    "# pip install elegy==0.8.5 # Because 0.8.6 has error.\n",
    "# pip install --upgrade \"jax[cuda12_local]==0.4.13\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# pip install tfp-nightly tensorflow_io tensorflow\n",
    "\n",
    "\"\"\"\n",
    "Train NN.\n",
    "Authored by Peter July 2023\n",
    "Edited by Linnea August/September 2023\n",
    "\n",
    "New Requirements:\n",
    "conda install python=3.9 numpy scipy pandas matplotlib\n",
    "conda install -c anaconda cudatoolkit\n",
    "pip install tensorflow\n",
    "pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import keras_core as keras\n",
    "\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "#import tensorflow as tf \n",
    "# #os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "# #os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.10'\n",
    "# #os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "# from jax import vmap\n",
    "# from jax import jit\n",
    "# from jax import grad\n",
    "# #assert jax.default_backend() == 'gpu'\n",
    "\n",
    "# import elegy # pip install elegy\n",
    "# import optax\n",
    "# import tensorflow_io as tfio # pip install tensorflow-io\n",
    "# #import tensorflow as tf # Recommended not to import this with jax because will also try to grab memory.\n",
    "# from tensorflow.data import Dataset # Trying not to import tf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 15:36:11.525145: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2023-08-29 15:36:11.525399: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 AVX512F FMA\n",
      "2023-08-29 15:36:11.787467: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 14683\n"
     ]
    }
   ],
   "source": [
    "# Create dataset object using IODataset\n",
    "polarity = 'pos' #'pos'\n",
    "path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "f = f'{path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "# features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(f, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(f, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(f, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y))\n",
    "train = full.take(np.floor(num_samples *.9))#.repeat()\n",
    "test = full.skip(np.floor(num_samples *.9))#.repeat()\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 128\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(num_samples * .9 / BATCH_SIZE )\n",
    "validation_steps = int(num_samples * .1 / BATCH_SIZE)\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "\n",
    "#train_x = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#train_y = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#x = train.map(lambda x,y: x)\n",
    "#y = train.map(lambda x,y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 15:36:13.430763: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b604c02abf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-29 15:36:13.430805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-08-29 15:36:13.476257: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-29 15:36:13.747292: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/linneamw/sadow_lts/personal/linneamw/anaconda3/envs/gcr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14683/14683 - 55s - 4ms/step - loss: 0.0016 - val_loss: 2.4655e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "14683/14683 - 52s - 4ms/step - loss: 1.8853e-04 - val_loss: 1.4227e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "14683/14683 - 56s - 4ms/step - loss: 1.1414e-04 - val_loss: 9.1052e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "14683/14683 - 57s - 4ms/step - loss: 7.5064e-05 - val_loss: 6.1310e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "14683/14683 - 56s - 4ms/step - loss: 5.3018e-05 - val_loss: 4.4690e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 3.9986e-05 - val_loss: 3.4751e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 3.2146e-05 - val_loss: 2.8631e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 2.7358e-05 - val_loss: 2.4891e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 2.4268e-05 - val_loss: 2.2447e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 2.2197e-05 - val_loss: 2.0794e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 2.0758e-05 - val_loss: 1.9676e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.9727e-05 - val_loss: 1.8871e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.8982e-05 - val_loss: 1.8364e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.8427e-05 - val_loss: 1.7987e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.8008e-05 - val_loss: 1.7718e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.7679e-05 - val_loss: 1.7551e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.7077e-05 - val_loss: 1.6730e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 18/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.6920e-05 - val_loss: 1.6601e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 19/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.6780e-05 - val_loss: 1.6489e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 20/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.6659e-05 - val_loss: 1.6393e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 21/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.6553e-05 - val_loss: 1.6319e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 22/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.6461e-05 - val_loss: 1.6254e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 23/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.6379e-05 - val_loss: 1.6187e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 24/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.6305e-05 - val_loss: 1.6123e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 25/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.6238e-05 - val_loss: 1.6074e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 26/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.6178e-05 - val_loss: 1.6024e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 27/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5966e-05 - val_loss: 1.5835e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 28/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.5933e-05 - val_loss: 1.5799e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 29/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5899e-05 - val_loss: 1.5767e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 30/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5867e-05 - val_loss: 1.5737e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 31/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5838e-05 - val_loss: 1.5708e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 32/100\n",
      "14683/14683 - 58s - 4ms/step - loss: 1.5809e-05 - val_loss: 1.5681e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 33/100\n",
      "14683/14683 - 61s - 4ms/step - loss: 1.5782e-05 - val_loss: 1.5656e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 34/100\n",
      "14683/14683 - 56s - 4ms/step - loss: 1.5757e-05 - val_loss: 1.5633e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 35/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.5733e-05 - val_loss: 1.5610e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 36/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5709e-05 - val_loss: 1.5588e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 37/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5605e-05 - val_loss: 1.5529e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 38/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5592e-05 - val_loss: 1.5514e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 39/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5578e-05 - val_loss: 1.5500e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 40/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5565e-05 - val_loss: 1.5486e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 41/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5552e-05 - val_loss: 1.5474e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 42/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5540e-05 - val_loss: 1.5461e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 43/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5528e-05 - val_loss: 1.5449e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 44/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5516e-05 - val_loss: 1.5437e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 45/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5504e-05 - val_loss: 1.5426e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 46/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5493e-05 - val_loss: 1.5414e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 47/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5434e-05 - val_loss: 1.5405e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 48/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5427e-05 - val_loss: 1.5398e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 49/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5421e-05 - val_loss: 1.5391e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 50/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.5414e-05 - val_loss: 1.5384e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 51/100\n",
      "14683/14683 - 55s - 4ms/step - loss: 1.5408e-05 - val_loss: 1.5378e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 52/100\n",
      "14683/14683 - 56s - 4ms/step - loss: 1.5401e-05 - val_loss: 1.5372e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 53/100\n",
      "14683/14683 - 57s - 4ms/step - loss: 1.5395e-05 - val_loss: 1.5365e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 54/100\n",
      "14683/14683 - 54s - 4ms/step - loss: 1.5389e-05 - val_loss: 1.5359e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 55/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5383e-05 - val_loss: 1.5353e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 56/100\n",
      "14683/14683 - 56s - 4ms/step - loss: 1.5377e-05 - val_loss: 1.5347e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 57/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5345e-05 - val_loss: 1.5334e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 58/100\n",
      "14683/14683 - 53s - 4ms/step - loss: 1.5342e-05 - val_loss: 1.5331e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 59/100\n"
     ]
    }
   ],
   "source": [
    "# Define model. \n",
    "l2=keras.regularizers.L2(l2=1e-6)\n",
    "model = keras.Sequential(layers=[\n",
    "   keras.layers.Input(shape=(8,)),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "# add tensorboard callback\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = f'../models/model_v1.0_{polarity}.keras'  # Must end with keras.\n",
    "log_dir = f'../tensorboard_logs/fit/model_v1.0_{polarity}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss'),\n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            ]\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    #steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\n",
    "    validation_data=test,\n",
    "    #validation_steps=1000,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step\n",
      "[[ 0.36066428  0.3642369   0.37132177  0.38184062  0.39962852  0.42059767\n",
      "   0.4450127   0.4758604   0.50966704  0.54539305  0.5860867   0.6265509\n",
      "   0.66512847  0.7015863   0.72998816  0.74664545  0.7489979   0.73533297\n",
      "   0.7083866   0.66809285  0.6181542   0.55204064  0.4763261   0.38666186\n",
      "   0.2848085   0.17946152  0.06527229 -0.05374647 -0.18028405 -0.3099442\n",
      "  -0.4432806  -0.5711507 ]] (1, 32)\n"
     ]
    }
   ],
   "source": [
    "# Test model load.\n",
    "model_path = '../models/model_2_256_selu_l21e-6.keras'  # Must end with keras.\n",
    "model2 = keras.models.load_model(model_path)\n",
    "x = np.random.rand(1,8)\n",
    "yhat = model.predict(x)\n",
    "print(yhat, yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions.\n",
    "\n",
    "# path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "# infile = f'{path}/pos/model_collection_1AU_90deg_0deg_fixed.h5'\n",
    "# outfile = f'{path}/pos/model_collection_1AU_90deg_0deg_fixed_processed.h5'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_hmc_elegy_oryx.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
