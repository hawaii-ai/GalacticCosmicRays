{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ve8yVrLbiOXv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 15:52:12.516771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 15:52:14.027912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Retraining NN with Elegy then do HMCon Mana\n",
    "# Author: Peter Nov 5 2022\n",
    "# Edited by Linnea August/September 2023\n",
    "\n",
    "# New Requirements:\n",
    "# conda install python=3.9 numpy scipy pandas matplotlib\n",
    "# conda install -c anaconda cudatoolkit\n",
    "# pip install tensorflow\n",
    "# pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\n",
    "# pip install elegy==0.8.5 # Because 0.8.6 has error.\n",
    "# pip install --upgrade \"jax[cuda12_local]==0.4.13\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# pip install tfp-nightly tensorflow_io tensorflow\n",
    "\n",
    "\"\"\"\n",
    "Train NN.\n",
    "Authored by Peter July 2023\n",
    "Edited by Linnea August/September 2023\n",
    "\n",
    "New Requirements:\n",
    "conda install python=3.9 numpy scipy pandas matplotlib\n",
    "conda install -c anaconda cudatoolkit\n",
    "pip install tensorflow\n",
    "pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import keras_core as keras\n",
    "\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "#import tensorflow as tf \n",
    "# #os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "# #os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.10'\n",
    "# #os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "# from jax import vmap\n",
    "# from jax import jit\n",
    "# from jax import grad\n",
    "# #assert jax.default_backend() == 'gpu'\n",
    "\n",
    "# import elegy # pip install elegy\n",
    "# import optax\n",
    "# import tensorflow_io as tfio # pip install tensorflow-io\n",
    "# #import tensorflow as tf # Recommended not to import this with jax because will also try to grab memory.\n",
    "# from tensorflow.data import Dataset # Trying not to import tf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 15:52:17.144161: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2023-09-27 15:52:17.144364: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 FMA\n",
      "2023-09-27 15:52:17.392395: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 14683\n"
     ]
    }
   ],
   "source": [
    "# Create dataset object using IODataset\n",
    "polarity = 'pos'\n",
    "path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "f = f'{path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "# features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(f, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(f, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(f, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y))\n",
    "train = full.take(np.floor(num_samples *.9))#.repeat()\n",
    "test = full.skip(np.floor(num_samples *.9))#.repeat()\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 128\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(num_samples * .9 / BATCH_SIZE )\n",
    "validation_steps = int(num_samples * .1 / BATCH_SIZE)\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "\n",
    "#train_x = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#train_y = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#x = train.map(lambda x,y: x)\n",
    "#y = train.map(lambda x,y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensorboard log dir:  ../../tensorboard_logs/fit/model_v3.0_pos/20230927-155217\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 15:52:19.148859: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b121c00f130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-27 15:52:19.148898: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-09-27 15:52:19.177846: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-27 15:52:19.506182: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/linneamw/sadow_lts/personal/linneamw/anaconda3/envs/gcr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14683/14683 - 59s - 4ms/step - loss: 0.0086 - val_loss: 0.0038 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "14683/14683 - 79s - 5ms/step - loss: 0.0035 - val_loss: 0.0030 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 0.0030 - val_loss: 0.0028 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "14683/14683 - 81s - 6ms/step - loss: 0.0028 - val_loss: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 0.0026 - val_loss: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 0.0025 - val_loss: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTensorboard log dir: \u001b[39m\u001b[39m\"\u001b[39m, log_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     train,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m#steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m#validation_steps=1000,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmana/home/linneamw/sadow_lts/personal/linneamw/research/gcr/GalacticCosmicRays/2023/train_nn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m )\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/gcr/lib/python3.9/site-packages/keras_core/src/utils/traceback_utils.py:119\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    121\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/gcr/lib/python3.9/site-packages/keras_core/src/backend/tensorflow/trainer.py:305\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mwith\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    304\u001b[0m     \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch():\n\u001b[0;32m--> 305\u001b[0m         callbacks\u001b[39m.\u001b[39;49mon_train_batch_begin(step)\n\u001b[1;32m    306\u001b[0m         logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    307\u001b[0m         callbacks\u001b[39m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    308\u001b[0m             step, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    309\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model. \n",
    "l2=keras.regularizers.L2(l2=1e-6)\n",
    "model = keras.Sequential(layers=[\n",
    "   keras.layers.Input(shape=(8,)),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "model_version = 'v3.0'\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = f'../models/model_{model_version}_{polarity}.keras'  # Must end with keras.\n",
    "log_dir = f'../../tensorboard_logs/fit/model_{model_version}_{polarity}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss'),\n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            ]\n",
    "model.compile(loss='mae', optimizer=optimizer)\n",
    "\n",
    "print(\"\\nTensorboard log dir: \", log_dir)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    #steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\n",
    "    validation_data=test,\n",
    "    #validation_steps=1000,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object using IODataset\n",
    "polarity = 'neg'\n",
    "path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "f = f'{path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "# features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(f, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(f, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(f, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y))\n",
    "train = full.take(np.floor(num_samples *.9))#.repeat()\n",
    "test = full.skip(np.floor(num_samples *.9))#.repeat()\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 128\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(num_samples * .9 / BATCH_SIZE )\n",
    "validation_steps = int(num_samples * .1 / BATCH_SIZE)\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "\n",
    "#train_x = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#train_y = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#x = train.map(lambda x,y: x)\n",
    "#y = train.map(lambda x,y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model. \n",
    "l2=keras.regularizers.L2(l2=1e-6)\n",
    "model = keras.Sequential(layers=[\n",
    "   keras.layers.Input(shape=(8,)),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "# add tensorboard callback\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_version = 'v3.0'\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = f'../models/model_{model_version}_{polarity}.keras'  # Must end with keras.\n",
    "log_dir = f'../../tensorboard_logs/fit/model_{model_version}_{polarity}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss'),\n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            ]\n",
    "model.compile(loss='mae', optimizer=optimizer)\n",
    "\n",
    "print(\"\\nTensorboard log dir: \", log_dir)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    #steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\n",
    "    validation_data=test,\n",
    "    #validation_steps=1000,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_hmc_elegy_oryx.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
