{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ve8yVrLbiOXv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 00:04:45.428405: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-13 00:04:45.432699: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-13 00:04:45.481617: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-13 00:04:45.484854: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 00:04:46.418526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Retraining NN with Elegy then do HMCon Mana\n",
    "# Author: Peter Nov 5 2022\n",
    "\n",
    "# New Requirements:\n",
    "# conda install python=3.9 numpy scipy pandas matplotlib\n",
    "# conda install -c anaconda cudatoolkit\n",
    "# pip install tensorflow\n",
    "# pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\n",
    "# pip install elegy==0.8.5 # Because 0.8.6 has error.\n",
    "# pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# pip install tfp-nightly tensorflow_io tensorflow\n",
    "\n",
    "\"\"\"\n",
    "Train NN.\n",
    "Peter July 2023\n",
    "\n",
    "New Requirements:\n",
    "conda install python=3.9 numpy scipy pandas matplotlib\n",
    "conda install -c anaconda cudatoolkit\n",
    "pip install tensorflow\n",
    "pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_core as keras\n",
    "\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "#import tensorflow as tf \n",
    "# #os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "# #os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.10'\n",
    "# #os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "# from jax import vmap\n",
    "# from jax import jit\n",
    "# from jax import grad\n",
    "# #assert jax.default_backend() == 'gpu'\n",
    "\n",
    "# import elegy # pip install elegy\n",
    "# import optax\n",
    "# import tensorflow_io as tfio # pip install tensorflow-io\n",
    "# #import tensorflow as tf # Recommended not to import this with jax because will also try to grab memory.\n",
    "# from tensorflow.data import Dataset # Trying not to import tf. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 00:04:48.687014: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2023-07-13 00:04:48.687147: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2023-07-13 00:04:48.924886: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 13975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataset object using IODataset\n",
    "polarity = 'neg'\n",
    "path = '/mnt/lts/nfs_fs02/sadow_lab/shared/gcr/data/2023_07_01/'\n",
    "f = f'{path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "#features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(f, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(f, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(f, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y))\n",
    "train = full.take(np.floor(num_samples *.9))#.repeat()\n",
    "test = full.skip(np.floor(num_samples *.9))#.repeat()\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 128\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(num_samples * .9 / BATCH_SIZE )\n",
    "validation_steps = int(num_samples * .1 / BATCH_SIZE)\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "\n",
    "#train_x = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#train_y = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#x = train.map(lambda x,y: x)\n",
    "#y = train.map(lambda x,y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 00:04:49.229330: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_32' with dtype int64 and shape [2]\n",
      "\t [[{{node Placeholder/_32}}]]\n",
      "2023-07-13 00:04:49.229993: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_10}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 00:04:49.766160: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x2b77c4028510 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-13 00:04:49.766183: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Host, Default Version\n",
      "2023-07-13 00:04:49.842116: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-13 00:04:50.188693: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/psadow/anaconda3/envs/gcr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2023-07-13 00:05:17.109152: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_10}}]]\n",
      "2023-07-13 00:05:17.109595: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_28' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_28}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13975/13975 - 33s - 2ms/step - loss: 0.0014 - val_loss: 3.0122e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "13975/13975 - 33s - 2ms/step - loss: 2.5170e-04 - val_loss: 2.0559e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 1.7996e-04 - val_loss: 1.5387e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 1.3656e-04 - val_loss: 1.1912e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 1.0742e-04 - val_loss: 9.5646e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 8.7409e-05 - val_loss: 7.9320e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "13975/13975 - 22s - 2ms/step - loss: 7.3320e-05 - val_loss: 6.7807e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "13975/13975 - 26s - 2ms/step - loss: 6.3235e-05 - val_loss: 5.9145e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 5.5882e-05 - val_loss: 5.2829e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "13975/13975 - 21s - 1ms/step - loss: 5.0463e-05 - val_loss: 4.8058e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 4.6339e-05 - val_loss: 4.4534e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 4.3125e-05 - val_loss: 4.1377e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 4.0597e-05 - val_loss: 3.8987e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 3.8599e-05 - val_loss: 3.7046e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 3.7010e-05 - val_loss: 3.5562e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 3.5740e-05 - val_loss: 3.4373e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "13975/13975 - 24s - 2ms/step - loss: 3.4719e-05 - val_loss: 3.3401e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "13975/13975 - 22s - 2ms/step - loss: 3.3883e-05 - val_loss: 3.2627e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 3.3192e-05 - val_loss: 3.2032e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 3.2037e-05 - val_loss: 3.1508e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 21/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 3.1682e-05 - val_loss: 3.1173e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 22/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 3.1370e-05 - val_loss: 3.0885e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 23/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 3.1096e-05 - val_loss: 3.0625e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 24/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 3.0854e-05 - val_loss: 3.0401e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 25/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 3.0638e-05 - val_loss: 3.0203e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 26/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 3.0443e-05 - val_loss: 3.0028e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 27/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 3.0265e-05 - val_loss: 2.9864e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 28/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 3.0102e-05 - val_loss: 2.9714e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 29/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.9951e-05 - val_loss: 2.9569e-05 - learning_rate: 5.0000e-05\n",
      "Epoch 30/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.9535e-05 - val_loss: 2.9298e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 31/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.9448e-05 - val_loss: 2.9201e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 32/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.9361e-05 - val_loss: 2.9112e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 33/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.9279e-05 - val_loss: 2.9030e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 34/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.9202e-05 - val_loss: 2.8952e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 35/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.9129e-05 - val_loss: 2.8881e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 36/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.9059e-05 - val_loss: 2.8813e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 37/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8991e-05 - val_loss: 2.8748e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 38/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8926e-05 - val_loss: 2.8687e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 39/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8864e-05 - val_loss: 2.8628e-05 - learning_rate: 2.5000e-05\n",
      "Epoch 40/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8668e-05 - val_loss: 2.8648e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 41/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8634e-05 - val_loss: 2.8611e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 42/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8597e-05 - val_loss: 2.8577e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 43/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 2.8561e-05 - val_loss: 2.8544e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 44/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8526e-05 - val_loss: 2.8511e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 45/100\n",
      "13975/13975 - 25s - 2ms/step - loss: 2.8492e-05 - val_loss: 2.8478e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 46/100\n",
      "13975/13975 - 22s - 2ms/step - loss: 2.8459e-05 - val_loss: 2.8449e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 47/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 2.8427e-05 - val_loss: 2.8419e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 48/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 2.8395e-05 - val_loss: 2.8389e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 49/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8362e-05 - val_loss: 2.8359e-05 - learning_rate: 1.2500e-05\n",
      "Epoch 50/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8257e-05 - val_loss: 2.8179e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 51/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8238e-05 - val_loss: 2.8160e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 52/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8220e-05 - val_loss: 2.8142e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 53/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8202e-05 - val_loss: 2.8125e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 54/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8184e-05 - val_loss: 2.8107e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 55/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8167e-05 - val_loss: 2.8090e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 56/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8150e-05 - val_loss: 2.8073e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 57/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8133e-05 - val_loss: 2.8057e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 58/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8116e-05 - val_loss: 2.8040e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 59/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8100e-05 - val_loss: 2.8024e-05 - learning_rate: 6.2500e-06\n",
      "Epoch 60/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8044e-05 - val_loss: 2.8009e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 61/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8034e-05 - val_loss: 2.8000e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 62/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.8025e-05 - val_loss: 2.7990e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 63/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8016e-05 - val_loss: 2.7981e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 64/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.8007e-05 - val_loss: 2.7972e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 65/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.7998e-05 - val_loss: 2.7963e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 66/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.7989e-05 - val_loss: 2.7954e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 67/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.7980e-05 - val_loss: 2.7945e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 68/100\n",
      "13975/13975 - 22s - 2ms/step - loss: 2.7971e-05 - val_loss: 2.7936e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 69/100\n",
      "13975/13975 - 22s - 2ms/step - loss: 2.7963e-05 - val_loss: 2.7928e-05 - learning_rate: 3.1250e-06\n",
      "Epoch 70/100\n",
      "13975/13975 - 21s - 2ms/step - loss: 2.7932e-05 - val_loss: 2.7981e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 71/100\n",
      "13975/13975 - 22s - 2ms/step - loss: 2.7927e-05 - val_loss: 2.7976e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 72/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.7922e-05 - val_loss: 2.7971e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 73/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7918e-05 - val_loss: 2.7966e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 74/100\n",
      "13975/13975 - 20s - 1ms/step - loss: 2.7913e-05 - val_loss: 2.7962e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 75/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7909e-05 - val_loss: 2.7957e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 76/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7904e-05 - val_loss: 2.7952e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 77/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7899e-05 - val_loss: 2.7948e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 78/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7895e-05 - val_loss: 2.7943e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 79/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7890e-05 - val_loss: 2.7938e-05 - learning_rate: 1.5625e-06\n",
      "Epoch 80/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7873e-05 - val_loss: 2.7868e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 81/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7870e-05 - val_loss: 2.7865e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 82/100\n",
      "13975/13975 - 18s - 1ms/step - loss: 2.7868e-05 - val_loss: 2.7863e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 83/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7866e-05 - val_loss: 2.7860e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 84/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7863e-05 - val_loss: 2.7858e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 85/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7861e-05 - val_loss: 2.7856e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 86/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7858e-05 - val_loss: 2.7853e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 87/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7856e-05 - val_loss: 2.7851e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 88/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7853e-05 - val_loss: 2.7848e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 89/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7851e-05 - val_loss: 2.7846e-05 - learning_rate: 7.8125e-07\n",
      "Epoch 90/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7842e-05 - val_loss: 2.7836e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 91/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7840e-05 - val_loss: 2.7834e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 92/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7839e-05 - val_loss: 2.7833e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 93/100\n",
      "13975/13975 - 18s - 1ms/step - loss: 2.7838e-05 - val_loss: 2.7832e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 94/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7836e-05 - val_loss: 2.7830e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 95/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7835e-05 - val_loss: 2.7829e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 96/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7834e-05 - val_loss: 2.7828e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 97/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7833e-05 - val_loss: 2.7827e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 98/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7831e-05 - val_loss: 2.7825e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 99/100\n",
      "13975/13975 - 19s - 1ms/step - loss: 2.7830e-05 - val_loss: 2.7824e-05 - learning_rate: 3.9062e-07\n",
      "Epoch 100/100\n"
     ]
    }
   ],
   "source": [
    "# Define model. \n",
    "l2=keras.regularizers.L2(l2=1e-6)\n",
    "model = keras.Sequential(layers=[\n",
    "   keras.layers.Input(shape=(8,)),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = f'../models/model_v1.0_{polarity}.keras'  # Must end with keras.\n",
    "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss'),\n",
    "            ]\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    #steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\n",
    "    validation_data=test,\n",
    "    #validation_steps=1000,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[[ 0.37365347  0.37716502  0.38417703  0.39466333  0.41205156  0.43274304\n",
      "   0.45662355  0.4868097   0.51950806  0.5541598   0.5928316   0.6309504\n",
      "   0.66670454  0.69956523  0.72432494  0.738734    0.7411985   0.7304528\n",
      "   0.707007    0.66976994  0.6221982   0.55842876  0.48455873  0.39619845\n",
      "   0.29504338  0.18976489  0.07533902 -0.04410985 -0.17151965 -0.30297035\n",
      "  -0.43793932 -0.57118696]] (1, 32)\n"
     ]
    }
   ],
   "source": [
    "# Test model load.\n",
    "model_path = '../models/model_2_256_selu_l21e-6.keras'  # Must end with keras.\n",
    "model2 = keras.models.load_model(model_path)\n",
    "x = np.random.rand(1,8)\n",
    "yhat = model.predict(x)\n",
    "print(yhat, yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "\n",
    "path = '/mnt/lts/nfs_fs02/sadow_lab/shared/gcr/data/2023_07_01/'\n",
    "infile = f'{path}/pos/model_collection_1AU_90deg_0deg_fixed.h5'\n",
    "outfile = f'{path}/pos/model_collection_1AU_90deg_0deg_fixed_processed.h5'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_hmc_elegy_oryx.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
