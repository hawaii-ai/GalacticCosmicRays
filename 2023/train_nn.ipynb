{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ve8yVrLbiOXv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retraining NN with Elegy then do HMCon Mana\n",
    "# Author: Peter Nov 5 2022\n",
    "# Edited by Linnea August/September 2023\n",
    "\n",
    "# New Requirements:\n",
    "# conda install python=3.9 numpy scipy pandas matplotlib\n",
    "# conda install -c anaconda cudatoolkit\n",
    "# pip install tensorflow\n",
    "# pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\n",
    "# pip install elegy==0.8.5 # Because 0.8.6 has error.\n",
    "# pip install --upgrade \"jax[cuda12_local]==0.4.13\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# pip install tfp-nightly tensorflow_io tensorflow\n",
    "\n",
    "\"\"\"\n",
    "Train NN.\n",
    "Authored by Peter July 2023\n",
    "Edited by Linnea August/September 2023\n",
    "\n",
    "New Requirements:\n",
    "conda install python=3.9 numpy scipy pandas matplotlib\n",
    "conda install -c anaconda cudatoolkit\n",
    "pip install tensorflow\n",
    "pip install tensorflow-io\\[tensorflow\\] # Seems to want specific older tf versions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import keras_core as keras\n",
    "\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "#import tensorflow as tf \n",
    "# #os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "# #os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.10'\n",
    "# #os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "# from jax import vmap\n",
    "# from jax import jit\n",
    "# from jax import grad\n",
    "# #assert jax.default_backend() == 'gpu'\n",
    "\n",
    "# import elegy # pip install elegy\n",
    "# import optax\n",
    "# import tensorflow_io as tfio # pip install tensorflow-io\n",
    "# #import tensorflow as tf # Recommended not to import this with jax because will also try to grab memory.\n",
    "# from tensorflow.data import Dataset # Trying not to import tf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 13:25:14.233604: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2023-09-11 13:25:14.233825: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 FMA\n",
      "2023-09-11 13:25:14.882543: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 14683\n"
     ]
    }
   ],
   "source": [
    "# Create dataset object using IODataset\n",
    "polarity = 'pos'\n",
    "path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "f = f'{path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "# features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(f, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(f, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(f, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y))\n",
    "train = full.take(np.floor(num_samples *.9))#.repeat()\n",
    "test = full.skip(np.floor(num_samples *.9))#.repeat()\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 128\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(num_samples * .9 / BATCH_SIZE )\n",
    "validation_steps = int(num_samples * .1 / BATCH_SIZE)\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "\n",
    "#train_x = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#train_y = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#x = train.map(lambda x,y: x)\n",
    "#y = train.map(lambda x,y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 13:25:16.850640: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b476000a5c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-11 13:25:16.850677: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-09-11 13:25:17.242646: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-11 13:25:17.725173: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/linneamw/sadow_lts/personal/linneamw/anaconda3/envs/gcr/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14683/14683 - 66s - 5ms/step - loss: 0.0014 - val_loss: 2.5656e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "14683/14683 - 77s - 5ms/step - loss: 2.0166e-04 - val_loss: 1.5683e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 1.2627e-04 - val_loss: 1.0027e-04 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 8.3193e-05 - val_loss: 6.7430e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 5.7795e-05 - val_loss: 4.8096e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 4.2596e-05 - val_loss: 3.6620e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 3.3547e-05 - val_loss: 2.9840e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 2.8070e-05 - val_loss: 2.5588e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 2.4625e-05 - val_loss: 2.2790e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 2.2396e-05 - val_loss: 2.0983e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "14683/14683 - 81s - 6ms/step - loss: 2.0912e-05 - val_loss: 1.9792e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "14683/14683 - 82s - 6ms/step - loss: 1.9899e-05 - val_loss: 1.8980e-05 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n"
     ]
    }
   ],
   "source": [
    "# Define model. \n",
    "l2=keras.regularizers.L2(l2=1e-6)\n",
    "model = keras.Sequential(layers=[\n",
    "   keras.layers.Input(shape=(8,)),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "# add tensorboard callback\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_version = 'v2.0'\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = f'../models/model_{model_version}_{polarity}.keras'  # Must end with keras.\n",
    "log_dir = f'../tensorboard_logs/fit/model_{model_version}_{polarity}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss'),\n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            ]\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "print(\"\\nTensorboard log dir: \", log_dir)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    #steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\n",
    "    validation_data=test,\n",
    "    #validation_steps=1000,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object using IODataset\n",
    "polarity = 'neg'\n",
    "path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "f = f'{path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "# features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(f, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(f, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(f, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y))\n",
    "train = full.take(np.floor(num_samples *.9))#.repeat()\n",
    "test = full.skip(np.floor(num_samples *.9))#.repeat()\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 128\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(num_samples * .9 / BATCH_SIZE )\n",
    "validation_steps = int(num_samples * .1 / BATCH_SIZE)\n",
    "print(f'Steps per epoch: {steps_per_epoch}')\n",
    "\n",
    "#train_x = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#train_y = TFDatasetAdapter(train.map(lambda x,y: x))\n",
    "#x = train.map(lambda x,y: x)\n",
    "#y = train.map(lambda x,y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model. \n",
    "l2=keras.regularizers.L2(l2=1e-6)\n",
    "model = keras.Sequential(layers=[\n",
    "   keras.layers.Input(shape=(8,)),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "   keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "# add tensorboard callback\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_version = 'v2.0'\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = f'../models/model_{model_version}_{polarity}.keras'  # Must end with keras.\n",
    "log_dir = f'../tensorboard_logs/fit/model_{model_version}_{polarity}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss'),\n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            ]\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "print(\"\\nTensorboard log dir: \", log_dir)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=100,\n",
    "    #steps_per_epoch=steps_per_epoch, #6 * 10000, #10000, # 10k*128 is approximate size of training set.\n",
    "    validation_data=test,\n",
    "    #validation_steps=1000,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step\n",
      "[[ 0.36066428  0.3642369   0.37132177  0.38184062  0.39962852  0.42059767\n",
      "   0.4450127   0.4758604   0.50966704  0.54539305  0.5860867   0.6265509\n",
      "   0.66512847  0.7015863   0.72998816  0.74664545  0.7489979   0.73533297\n",
      "   0.7083866   0.66809285  0.6181542   0.55204064  0.4763261   0.38666186\n",
      "   0.2848085   0.17946152  0.06527229 -0.05374647 -0.18028405 -0.3099442\n",
      "  -0.4432806  -0.5711507 ]] (1, 32)\n"
     ]
    }
   ],
   "source": [
    "# # Test model load.\n",
    "# model_path = '../models/model_2_256_selu_l21e-6.keras'  # Must end with keras.\n",
    "# model2 = keras.models.load_model(model_path)\n",
    "# x = np.random.rand(1,8)\n",
    "# yhat = model.predict(x)\n",
    "# print(yhat, yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions.\n",
    "\n",
    "# path = '/home/linneamw/sadow_lts/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "# infile = f'{path}/pos/model_collection_1AU_90deg_0deg_fixed.h5'\n",
    "# outfile = f'{path}/pos/model_collection_1AU_90deg_0deg_fixed_processed.h5'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_hmc_elegy_oryx.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
