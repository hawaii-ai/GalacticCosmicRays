{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d0bf14",
   "metadata": {},
   "source": [
    "# Evaluate NN performance over training sizes\n",
    "Simple dev notebook to evaluate the performance of a neural network over different training sizes. The goal is to understand how the model's performance changes as we increase the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f61409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import h5py\n",
    "import numpy as np\n",
    "# import keras_core as keras\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "import os\n",
    "os.sys.path.append(\"/home/linneamw/sadow_koastore/personal/linneamw/research/gcr/GalacticCosmicRays/scripts/nn_train_size_analysis\")\n",
    "from rtdl_num_embeddings_tf import (\n",
    "    LinearEmbeddings,\n",
    "    LinearReLUEmbeddings,\n",
    "    PeriodicEmbeddings,\n",
    "    PiecewiseLinearEmbeddings,\n",
    "    compute_bins,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782ad724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (32, 10)\n",
      "Linear embeddings shape: (32, 10, 32)\n",
      "Periodic embeddings shape: (32, 10, 64)\n",
      "Piecewise linear embeddings shape: (32, 10, 32)\n"
     ]
    }
   ],
   "source": [
    "# Test rtdl_num_embeddings_tf\n",
    "B, F = 32, 10\n",
    "x = tf.random.normal((B, F))\n",
    "\n",
    "# Linear embeddings\n",
    "emb = LinearEmbeddings(n_features=F, d_embedding=32)\n",
    "y = emb(x)  # (B, F, 32)\n",
    "\n",
    "# Periodic embeddings\n",
    "pemb = PeriodicEmbeddings(n_features=F, d_embedding=64, k=64, sigma=0.02, activation=True, version=\"B\")\n",
    "yp = pemb(x)  # (B, F, 64)\n",
    "\n",
    "# Piecewise: compute quantile bins then encode\n",
    "bins = compute_bins(x, n_bins=32)        # list of length F, per-feature edges\n",
    "pe  = PiecewiseLinearEmbeddings(bins, d_embedding=32, activation=True, version=\"A\")\n",
    "ype = pe(x)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Linear embeddings shape:\", y.shape)\n",
    "print(\"Periodic embeddings shape:\", yp.shape)\n",
    "print(\"Piecewise linear embeddings shape:\", ype.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd107a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(polarity, data_version, train_size_fraction, bootstrap):\n",
    "    # 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "    # features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "    data_path = '/home/linneamw/sadow_koastore/personal/linneamw/research/gcr/data/shuffled_may2025'\n",
    "    train_file = f'{data_path}/{polarity}/train.h5'\n",
    "    test_file = f'{data_path}/{polarity}/test.h5'\n",
    "\n",
    "    # Load train data\n",
    "    with h5py.File(train_file, 'r') as h5:\n",
    "        num_train_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "        _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "    x_train = tfio.IODataset.from_hdf5(train_file, dataset='/X_minmax')\n",
    "    y_train = tfio.IODataset.from_hdf5(train_file, dataset='/Y_log_scaled')\n",
    "    full_train = Dataset.zip((x_train, y_train))\n",
    "\n",
    "    # Load test data\n",
    "    with h5py.File(test_file, 'r') as h5:\n",
    "        num_test_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "        _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "    x_test = tfio.IODataset.from_hdf5(test_file, dataset='/X_minmax')\n",
    "    y_test = tfio.IODataset.from_hdf5(test_file, dataset='/Y_log_scaled')\n",
    "    test = Dataset.zip((x_test, y_test))\n",
    "\n",
    "    # Get number of training samples (from the dataset)\n",
    "    train_size = int(np.floor(num_train_samples * train_size_fraction))\n",
    "    print(f\"Number of training samples: {train_size} out of {num_train_samples} total\")\n",
    "    print(f\"Number of test samples: {num_test_samples}\")\n",
    "\n",
    "    # Choose seed based on model version\n",
    "    data_seeds = {\n",
    "        'd1': 42,\n",
    "        'd2': 87,\n",
    "        'd3': 5,\n",
    "        'd4': 98,\n",
    "        'd5': 123,\n",
    "    }\n",
    "    data_seed = data_seeds.get(data_version, None)\n",
    "\n",
    "    if bootstrap == 'b1':\n",
    "        # Reproducible bootstrap indices\n",
    "        rng = np.random.default_rng(data_seed)\n",
    "        sampled_indices = rng.integers(low=0, high=num_train_samples, size=train_size)\n",
    "\n",
    "        # Load dataset into memory\n",
    "        train_list = list(full_train.as_numpy_iterator())\n",
    "\n",
    "        # Sample with replacement\n",
    "        bootstrapped_data = [train_list[i] for i in sampled_indices]\n",
    "\n",
    "        # Separate into inputs and outputs\n",
    "        x_bootstrap, y_bootstrap = zip(*bootstrapped_data)\n",
    "\n",
    "        # Convert back to tf.data.Dataset\n",
    "        train = Dataset.from_tensor_slices((list(x_bootstrap), list(y_bootstrap)))\n",
    "\n",
    "    else:\n",
    "        # Shuffle deterministically\n",
    "        if data_version in data_seeds:\n",
    "            train_shuffled = full_train.shuffle(\n",
    "                buffer_size=num_train_samples, seed=data_seed, reshuffle_each_iteration=False\n",
    "            )\n",
    "        else:\n",
    "            train_shuffled = full_train\n",
    "\n",
    "        # Take subset without replacement\n",
    "        train = train_shuffled.take(train_size)\n",
    "\n",
    "    # Set batch_size to 128 unless the train size is smaller than 128, then set it to the train size.\n",
    "    if train_size < 128:\n",
    "        batch_size = train_size\n",
    "    else:\n",
    "        batch_size = 128\n",
    "\n",
    "    train = train.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "    test = test.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train, test, train_size, num_test_samples, batch_size, num_inputs\n",
    "\n",
    "def build_model(input_dim, n_layers, units, embedding_method, embed_dim=12, n_bins=48):\n",
    "    print(f\"Building model with embedding {embedding_method}, {n_layers} layers, and {units} units per layer\")\n",
    "\n",
    "    model = keras.Sequential([keras.Input(shape=(input_dim,), dtype=\"float32\")])\n",
    "\n",
    "    # Tabular embedding layer\n",
    "    if embedding_method == \"linear\":\n",
    "        model.add(LinearEmbeddings(input_dim, embed_dim))\n",
    "        model.add(keras.layers.Flatten())\n",
    "    elif embedding_method == \"linear_relu\":\n",
    "        model.add(LinearReLUEmbeddings(input_dim, embed_dim))\n",
    "        model.add(keras.layers.Flatten())\n",
    "    elif embedding_method == \"periodic\":\n",
    "        # Defaults: k=64, sigma=0.02, activation=True (you can change)\n",
    "        model.add(PeriodicEmbeddings(input_dim, embed_dim))\n",
    "        model.add(keras.layers.Flatten())\n",
    "    # TODO: fix this\n",
    "    # elif embedding_method in {\"piecewise_linear\", \"piecewise_linear_relu\"}:\n",
    "    #     # Compute bins **once** outside the training loop; pass numpy or a dense tensor\n",
    "    #     bins = compute_bins(x_train, n_bins)\n",
    "    #     model.add(PiecewiseLinearEmbeddings(\n",
    "    #         bins, embed_dim,\n",
    "    #         activation=(embedding_method == \"piecewise_linear_relu\"),\n",
    "    #         version=\"B\"  # residual linear, as in your code\n",
    "    #     ))\n",
    "    #     model.add(keras.layers.Flatten())\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown embedding_method: {embedding_method}\")\n",
    "\n",
    "    # If you’re using SELU, pair with lecun_normal + AlphaDropout (recommended for SELU)\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "        # optional:\n",
    "        # model.add(keras.layers.AlphaDropout(0.05))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261ea137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 178889 out of 1788892 total\n",
      "Number of test samples: 198766\n",
      "Building model with embedding periodic, 2 layers, and 1024 units per layer\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " periodic_embeddings_3 (Per  (None, 8, 12)             12896     \n",
      " iodicEmbeddings)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_3 (Dense)             (None, 1024)              99328     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                32800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1194624 (4.56 MB)\n",
      "Trainable params: 1194624 (4.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Fixed args – customize if needed\n",
    "args = {\n",
    "    \"polarity\": \"neg\",\n",
    "    \"train_size_fraction\": 0.1,\n",
    "    \"bootstrap\": \"b0\",\n",
    "    \"data_version\": \"d1\",\n",
    "    \"n_layers\": 2,\n",
    "    \"units\": 1024,\n",
    "    \"embedding_method\": \"periodic\", # Options: none, linear_relu, periodic, piecewise_linear_relu\n",
    "    \"learning_rate\": 1.918416336823577e-05,\n",
    "    \"weight_decay\": 3.251785236175247e-06\n",
    "}\n",
    "\n",
    "train, test, train_size, num_test_samples, batch_size, num_inputs = load_dataset(\n",
    "    args[\"polarity\"], args[\"data_version\"], args[\"train_size_fraction\"], args[\"bootstrap\"]\n",
    ")\n",
    "\n",
    "# # Get x_final_train from the zipped and shuffled and batched train\n",
    "# # Collect all batches into memory\n",
    "# x_batches = []\n",
    "# for x_batch, _ in train:   # iterate through the dataset\n",
    "#     x_batches.append(x_batch.numpy())  # convert to numpy\n",
    "\n",
    "# # Concatenate into a single array\n",
    "# x_final_train = np.concatenate(x_batches, axis=0)\n",
    "# x_final_train_tensor = tf.convert_to_tensor(x_final_train, dtype=tf.float32)\n",
    "\n",
    "# Define model\n",
    "model = build_model(num_inputs, args[\"n_layers\"], args[\"units\"], args[\"embedding_method\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828253a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 15:58:42.975046: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12388095878121654777\n",
      "2025-09-08 15:58:42.975159: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 802256284724381102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397/1397 - 16s - loss: 0.0386 - mse: 0.0087 - val_loss: 0.0152 - val_mse: 6.5556e-04 - lr: 1.9184e-05 - 16s/epoch - 11ms/step\n",
      "Epoch 2/10\n",
      "1397/1397 - 11s - loss: 0.0138 - mse: 5.7485e-04 - val_loss: 0.0124 - val_mse: 5.0422e-04 - lr: 1.9184e-05 - 11s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "1397/1397 - 10s - loss: 0.0116 - mse: 4.4578e-04 - val_loss: 0.0107 - val_mse: 3.9535e-04 - lr: 1.9184e-05 - 10s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "1397/1397 - 9s - loss: 0.0104 - mse: 3.6566e-04 - val_loss: 0.0100 - val_mse: 3.3991e-04 - lr: 1.9184e-05 - 9s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "1397/1397 - 12s - loss: 0.0095 - mse: 3.1794e-04 - val_loss: 0.0093 - val_mse: 3.0115e-04 - lr: 1.9184e-05 - 12s/epoch - 9ms/step\n",
      "Epoch 6/10\n",
      "1397/1397 - 9s - loss: 0.0091 - mse: 2.8649e-04 - val_loss: 0.0088 - val_mse: 2.7408e-04 - lr: 1.9184e-05 - 9s/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "1397/1397 - 10s - loss: 0.0087 - mse: 2.6183e-04 - val_loss: 0.0086 - val_mse: 2.5616e-04 - lr: 1.9184e-05 - 10s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "1397/1397 - 9s - loss: 0.0083 - mse: 2.4117e-04 - val_loss: 0.0080 - val_mse: 2.3137e-04 - lr: 1.9184e-05 - 9s/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "1397/1397 - 10s - loss: 0.0081 - mse: 2.2461e-04 - val_loss: 0.0075 - val_mse: 2.1213e-04 - lr: 1.9184e-05 - 10s/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "1397/1397 - 10s - loss: 0.0078 - mse: 2.0904e-04 - val_loss: 0.0075 - val_mse: 2.0171e-04 - lr: 1.9184e-05 - 10s/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=args[\"learning_rate\"], weight_decay=args[\"weight_decay\"])\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mse'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=10,\n",
    "    validation_data=test,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b766ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:00:51.721511: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11988745245769156963\n",
      "2025-09-08 16:00:51.721696: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15565479340471374195\n",
      "2025-09-08 16:00:58.699620: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11988745245769156963\n",
      "2025-09-08 16:00:58.699706: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15565479340471374195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.00745767168700695, Train MSE: 0.00020026136189699173\n",
      "Test MAE: 0.007487054448574781, Test MSE: 0.00020170820062048733\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on train\n",
    "train_results = model.evaluate(train, verbose=0)\n",
    "train_mae = train_results[0]  # This is the loss (MAE)\n",
    "train_mse = train_results[1]  # This is the metric (MSE)\n",
    "\n",
    "# Evaluate on test\n",
    "test_results = model.evaluate(test, verbose=0)\n",
    "test_mae = test_results[0]\n",
    "test_mse = test_results[1]\n",
    "\n",
    "print(f\"Train MAE: {train_mae}, Train MSE: {train_mse}\")\n",
    "print(f\"Test MAE: {test_mae}, Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb0f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
