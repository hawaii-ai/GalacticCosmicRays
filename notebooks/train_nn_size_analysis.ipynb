{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d0bf14",
   "metadata": {},
   "source": [
    "# Evaluate NN performance over training sizes\n",
    "Simple dev notebook to evaluate the performance of a neural network over different training sizes. The goal is to understand how the model's performance changes as we increase the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f61409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 13:51:33.226804: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-06 13:51:33.291359: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-06 13:51:33.294165: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-06 13:51:36.191958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import keras_core as keras\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "SPLIT_SEED = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145d8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "polarity = 'neg'\n",
    "train_size_fraction = 0.1\n",
    "bootstrap = 'b1' # 'b1' = True, 'b0' = False\n",
    "model_version = 'init0'\n",
    "data_version = 'd1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd107a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 13:52:56.866691: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2025-05-06 13:52:56.867033: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 AVX512F FMA\n",
      "2025-05-06 13:52:57.056702: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 178889 = 0.1 * 1788892.0\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/linneamw/sadow_koastore/personal/linneamw/research/gcr/data/2023_07_01'\n",
    "data_file = f'{data_path}/{polarity}/model_collection_1AU_90deg_0deg_fixed_training.h5'\n",
    "    \n",
    "# 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "# features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "with h5py.File(data_file, 'r') as h5:\n",
    "    num_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "    _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "x = tfio.IODataset.from_hdf5(data_file, dataset='/X_minmax')\n",
    "y = tfio.IODataset.from_hdf5(data_file, dataset='/Y_log_scaled')\n",
    "\n",
    "# Split\n",
    "full = Dataset.zip((x, y)).shuffle(\n",
    "    buffer_size=num_samples, seed=SPLIT_SEED, reshuffle_each_iteration=False\n",
    ") # Shuffle the dataset. Important to use the same seed and buffer_size\n",
    "train_cardinality = np.floor(num_samples * .9)\n",
    "full_train = full.take(train_cardinality) # Keep train set we sample from consistent as 90% of the data\n",
    "test = full.skip(train_cardinality) # Keep test set consistent as 10% of the data\n",
    "\n",
    "# Get number of training samples (from the dataset)\n",
    "train_size = int(np.floor(train_cardinality * train_size_fraction))\n",
    "print(f'Train size: {train_size} = {train_size_fraction} * {train_cardinality}')\n",
    "\n",
    "# Choose seed based on model version\n",
    "data_seeds = {\n",
    "    'd1': 42,\n",
    "    'd2': 87,\n",
    "    'd3': 5,\n",
    "    'd4': 98,\n",
    "}\n",
    "data_seed = data_seeds.get(data_version, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ea137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bootstrap sampling (with replacement) for data version d1 and seed 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 13:53:26.248651: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 1 of 1987658\n",
      "2025-05-06 13:53:26.248717: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2 of 1987658\n",
      "2025-05-06 13:53:30.643172: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 1025 of 1987658\n",
      "2025-05-06 13:53:40.902952: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 7169 of 1987658\n",
      "2025-05-06 13:53:47.672107: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 173324 of 1987658\n",
      "2025-05-06 13:53:53.800744: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "if bootstrap == 'b1':\n",
    "    print(f\"Using bootstrap sampling (with replacement) for data version {data_version} and seed {data_seed}\")\n",
    "\n",
    "    # Reproducible bootstrap indices\n",
    "    rng = np.random.default_rng(data_seed)\n",
    "    sampled_indices = rng.integers(low=0, high=train_cardinality, size=train_size)\n",
    "\n",
    "    # Load dataset into memory\n",
    "    train_list = list(full_train.as_numpy_iterator())\n",
    "\n",
    "    # Sample with replacement\n",
    "    bootstrapped_data = [train_list[i] for i in sampled_indices]\n",
    "\n",
    "    # Separate into inputs and outputs\n",
    "    x_bootstrap, y_bootstrap = zip(*bootstrapped_data)\n",
    "\n",
    "    # Convert back to tf.data.Dataset\n",
    "    train = Dataset.from_tensor_slices((list(x_bootstrap), list(y_bootstrap)))\n",
    "\n",
    "else:\n",
    "    print(f\"Using traditional sampling (without replacement) for data version {data_version} and seed {data_seed}\")\n",
    "\n",
    "    # Shuffle deterministically\n",
    "    if data_version in data_seeds:\n",
    "        train_shuffled = full_train.shuffle(\n",
    "            buffer_size=train_cardinality, seed=data_seed, reshuffle_each_iteration=False\n",
    "        )\n",
    "    else:\n",
    "        train_shuffled = full_train\n",
    "\n",
    "    # Take subset without replacement\n",
    "    train = train_shuffled.take(train_size)\n",
    "\n",
    "# Set batch_size to 128 unless the train size is smaller than 128, then set it to the train size.\n",
    "if train_size < 128:\n",
    "    batch_size = train_size\n",
    "else:\n",
    "    batch_size = 128\n",
    "print(f'Setting batch size: {batch_size}')\n",
    "\n",
    "train = train.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "test = test.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "# Some calcs\n",
    "steps_per_epoch = int(train_size / batch_size )\n",
    "validation_steps = int(num_samples * .1 / batch_size)\n",
    "print(f'Steps per epoch: {steps_per_epoch}, validation steps: {validation_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cde55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensorboard log dir:  ../../tensorboard_logs/regularized/data_d1_bootstrap_b1_model_init0_train_size_0.5_neg/20250506-124827\n"
     ]
    }
   ],
   "source": [
    "# Define model. \n",
    "l2 = keras.regularizers.L2(l2=1e-2)\n",
    "model = keras.Sequential(layers=[\n",
    "    keras.layers.Input(shape=(8,)),\n",
    "    keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "    keras.layers.Dense(256, activation='selu', kernel_regularizer=l2),\n",
    "    keras.layers.Dense(32, activation='linear', kernel_regularizer=l2),\n",
    "])\n",
    "\n",
    "# Create save and log directories\n",
    "save_ending_name = 'regularized'\n",
    "save_name = f'data_{data_version}_bootstrap_{bootstrap}_model_{model_version}_train_size_{train_size_fraction}_{polarity}'\n",
    "log_dir = f'../../tensorboard_logs/{save_ending_name}/{save_name}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "print(\"\\nTensorboard log dir: \", log_dir)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828253a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(loss='mae', optimizer=optimizer)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae = model.evaluate(train)\n",
    "test_mae = model.evaluate(test)\n",
    "\n",
    "print(f'Train MAE: {train_mae}, Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb0f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
