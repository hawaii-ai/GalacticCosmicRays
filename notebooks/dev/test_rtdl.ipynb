{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5873562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 20:53:27.639325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-14 20:53:27.639380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-14 20:53:27.640727: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-14 20:53:27.651168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-14 20:53:30.322774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "from typing import Optional, Sequence, Tuple\n",
    "import math\n",
    "import keras_core as keras\n",
    "from keras_core import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../../scripts/nn_train_size_analysis/')\n",
    "from rtdl_num_embeddings_keras import (\n",
    "    PeriodicEmbeddings,\n",
    "    PiecewiseLinearEmbeddings,\n",
    "    PiecewiseLinearEncoding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327820ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.2061374 -1.6543508 12.153855  -1.2604606  0.5480492]]\n"
     ]
    }
   ],
   "source": [
    "# Deterministic RNG across backends\n",
    "try:\n",
    "    keras.utils.set_random_seed(42)\n",
    "except Exception:\n",
    "    pass\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "n_features = 5\n",
    "batch = 1\n",
    "\n",
    "x = 5 * np.random.randn(batch, n_features).astype(\"float32\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af309165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.02169013023376465 seconds\n",
      "tf.Tensor(\n",
      "[[ 1.          1.          1.          1.2061377   0.34564924  0.\n",
      "   0.          0.          1.          1.          1.         11.153855\n",
      "   0.7395394   0.          0.          0.          1.          1.\n",
      "   0.5480492   0.        ]], shape=(1, 20), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 1.          1.          1.          1.2061377 ]\n",
      "  [ 0.34564924  0.          0.          0.        ]\n",
      "  [ 1.          1.          1.         11.153855  ]\n",
      "  [ 0.7395394   0.          0.          0.        ]\n",
      "  [ 1.          1.          0.5480492   0.        ]]], shape=(1, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test PiecewiseLinearEncoding on x with a set value_range\n",
    "range = (-2.0, 2.0)\n",
    "mins = np.ones(n_features, dtype=\"float32\") * range[0]\n",
    "maxs = np.ones(n_features, dtype=\"float32\") * range[1]\n",
    "value_range = (mins, maxs)\n",
    "n_features = x.shape[1]\n",
    "n_bins = 4\n",
    "\n",
    "x_tensor = ops.convert_to_tensor(x)\n",
    "\n",
    "import time\n",
    "ple = PiecewiseLinearEncoding(\n",
    "    value_range=value_range,\n",
    "    n_features=n_features,\n",
    "    n_bins=n_bins,\n",
    "    use_adaptive_range=False,\n",
    "    clip=False,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y = ple(x_tensor)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "print(y)\n",
    "\n",
    "# Undo the concatenation of shape (batch, n_features * n_bins)\n",
    "y = tf.reshape(y, (batch, n_features, n_bins))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3686070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeriodicEmbeddings forward OK: (1, 80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 20:29:29.757231: I external/local_xla/xla/service/service.cc:168] XLA service 0x15371c054670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-14 20:29:29.757256: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-10-14 20:29:29.823051: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-14 20:29:30.355620: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760509770.746177 1203056 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeriodicEmbeddings training step OK, loss: 0.8265498280525208\n",
      "PeriodicEmbeddings serialization OK\n"
     ]
    }
   ],
   "source": [
    "# ===== PeriodicEmbeddings: shape + forward =====\n",
    "per = PeriodicEmbeddings(\n",
    "    n_features=n_features,\n",
    "    n_frequencies=8,\n",
    "    learnable_frequencies=True,\n",
    "    use_phase=True,\n",
    "    learnable_phases=True,\n",
    ")\n",
    "y_per = per(x)\n",
    "expected_per = n_features * 8 * 2\n",
    "assert y_per.shape == (batch, expected_per), f\"Periodic shape {y_per.shape} != (B,{expected_per})\"\n",
    "print(\"PeriodicEmbeddings forward OK:\", y_per.shape)\n",
    "\n",
    "# Trainability check by fitting a tiny head\n",
    "inp = keras.Input((n_features,), dtype=\"float32\")\n",
    "out = per(inp)\n",
    "out = keras.layers.Dense(4, activation=\"relu\")(out)\n",
    "model_per = keras.Model(inp, out)\n",
    "model_per.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "y_target = np.random.randn(batch, 4).astype(\"float32\")\n",
    "hist = model_per.fit(x, y_target, epochs=2, verbose=0)\n",
    "assert np.isfinite(hist.history[\"loss\"][ -1 ]), \"NaN loss in PeriodicEmbeddings model\"\n",
    "print(\"PeriodicEmbeddings training step OK, loss:\", hist.history[\"loss\"][ -1 ])\n",
    "\n",
    "# Serialization round-trip\n",
    "model_per.save(\"per_test.keras\")\n",
    "model_per_loaded = keras.models.load_model(\n",
    "    \"per_test.keras\", custom_objects={\"PeriodicEmbeddings\": PeriodicEmbeddings}\n",
    ")\n",
    "p1 = model_per.predict(x, verbose=0)\n",
    "p2 = model_per_loaded.predict(x, verbose=0)\n",
    "assert np.allclose(p1, p2, atol=1e-5), \"Loaded Periodic model predictions differ\"\n",
    "print(\"PeriodicEmbeddings serialization OK\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e772e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PiecewiseLinearEncoding forward OK: (1, 80)\n",
      "PiecewiseLinearEncoding training step OK, loss: 0.5233413577079773\n",
      "PiecewiseLinearEncoding serialization OK\n"
     ]
    }
   ],
   "source": [
    "# ===== PiecewiseLinearEncoding (fixed): adapt + forward =====\n",
    "ple = PiecewiseLinearEncoding(\n",
    "    n_features=n_features,\n",
    "    n_bins=16,\n",
    "    use_adaptive_range=True,\n",
    "    clip=True,\n",
    ")\n",
    "ple.adapt(x)\n",
    "y_ple = ple(x)\n",
    "expected_ple = n_features * 16\n",
    "assert y_ple.shape == (batch, expected_ple), f\"PLE shape {y_ple.shape} != (B,{expected_ple})\"\n",
    "print(\"PiecewiseLinearEncoding forward OK:\", y_ple.shape)\n",
    "\n",
    "# PLE can feed an MLP head\n",
    "inp2 = keras.Input((n_features,), dtype=\"float32\")\n",
    "out2 = ple(inp2)\n",
    "out2 = keras.layers.Dense(3)(out2)\n",
    "model_ple = keras.Model(inp2, out2)\n",
    "model_ple.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "y2 = np.random.randn(batch, 3).astype(\"float32\")\n",
    "hist2 = model_ple.fit(x, y2, epochs=2, verbose=0)\n",
    "assert np.isfinite(hist2.history[\"loss\"][ -1 ]), \"NaN loss in PLE model\"\n",
    "print(\"PiecewiseLinearEncoding training step OK, loss:\", hist2.history[\"loss\"][ -1 ])\n",
    "\n",
    "# Serialization round-trip (PLE)\n",
    "model_ple.save(\"ple_fixed_test.keras\")\n",
    "model_ple_loaded = keras.models.load_model(\n",
    "    \"ple_fixed_test.keras\",\n",
    "    custom_objects={\"PiecewiseLinearEncoding\": PiecewiseLinearEncoding}\n",
    ")\n",
    "q1 = model_ple.predict(x, verbose=0)\n",
    "q2 = model_ple_loaded.predict(x, verbose=0)\n",
    "assert np.allclose(q1, q2, atol=1e-5), \"Loaded PLE model predictions differ\"\n",
    "print(\"PiecewiseLinearEncoding serialization OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e9afed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PiecewiseLinearEmbeddings forward OK: (1, 40)\n",
      "PiecewiseLinearEmbeddings training step OK, loss: 1.0864019393920898\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x153741ba6160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x153741aedee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "PiecewiseLinearEmbeddings serialization OK\n"
     ]
    }
   ],
   "source": [
    "# ===== PiecewiseLinearEmbeddings: adapt + forward =====\n",
    "plemb = PiecewiseLinearEmbeddings(\n",
    "    n_features=n_features,\n",
    "    n_bins=16,\n",
    "    d_embedding=8,\n",
    "    activation=True,          # ReLU(Linear(PLE))\n",
    "    use_adaptive_range=True,\n",
    "    clip=True,\n",
    ")\n",
    "plemb.adapt(x)\n",
    "y_plemb = plemb(x)\n",
    "expected_plemb = n_features * 8\n",
    "assert y_plemb.shape == (batch, expected_plemb), f\"PLEmb shape {y_plemb.shape} != (B,{expected_plemb})\"\n",
    "print(\"PiecewiseLinearEmbeddings forward OK:\", y_plemb.shape)\n",
    "\n",
    "# Trainability + compile/fit for embeddings variant\n",
    "inp3 = keras.Input((n_features,), dtype=\"float32\")\n",
    "out3 = plemb(inp3)\n",
    "out3 = keras.layers.Dense(3)(out3)\n",
    "model_plemb = keras.Model(inp3, out3)\n",
    "model_plemb.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "y3 = np.random.randn(batch, 3).astype(\"float32\")\n",
    "hist3 = model_plemb.fit(x, y3, epochs=2, verbose=0)\n",
    "assert np.isfinite(hist3.history[\"loss\"][ -1 ]), \"NaN loss in PLEmb model\"\n",
    "print(\"PiecewiseLinearEmbeddings training step OK, loss:\", hist3.history[\"loss\"][ -1 ])\n",
    "\n",
    "# Serialization round-trip (PLEmb)\n",
    "model_plemb.save(\"plemb_test.keras\")\n",
    "model_plemb_loaded = keras.models.load_model(\n",
    "    \"plemb_test.keras\",\n",
    "    custom_objects={\n",
    "        \"PiecewiseLinearEmbeddings\": PiecewiseLinearEmbeddings,\n",
    "        \"PiecewiseLinearEncoding\": PiecewiseLinearEncoding,\n",
    "    }\n",
    ")\n",
    "r1 = model_plemb.predict(x, verbose=0)\n",
    "r2 = model_plemb_loaded.predict(x, verbose=0)\n",
    "assert np.allclose(r1, r2, atol=1e-5), \"Loaded PLEmb model predictions differ\"\n",
    "print(\"PiecewiseLinearEmbeddings serialization OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b70914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All quick tests passed ✔\n"
     ]
    }
   ],
   "source": [
    "print(\"All quick tests passed ✔\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a425a",
   "metadata": {},
   "source": [
    "# Try building a model and passing data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d605cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linneamw/anaconda3/envs/gcr-gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import h5py\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import keras_core as keras\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../../scripts/nn_train_size_analysis/')\n",
    "from rtdl_num_embeddings_keras import (\n",
    "    PeriodicEmbeddings,\n",
    "    PiecewiseLinearEncoding,\n",
    "    PiecewiseLinearEmbeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d820c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(polarity, data_version, train_size_fraction, bootstrap):\n",
    "    # 8 input parameters for the NN: alpha, cmf, vspoles, cpa, pwr1par, pwr2par, pwr1perr, and pwr2perr.\n",
    "    # features = ['alpha', 'cmf', 'cpa', 'pwr1par', 'pwr1perr', 'pwr2par', 'pwr2perr', 'vspoles']\n",
    "    data_path = '/home/linneamw/sadow_koastore/personal/linneamw/research/gcr/data/shuffled_may2025'\n",
    "    train_file = f'{data_path}/{polarity}/train.h5'\n",
    "    test_file = f'{data_path}/{polarity}/test.h5'\n",
    "\n",
    "    # Load train data\n",
    "    with h5py.File(train_file, 'r') as h5:\n",
    "        num_train_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "        _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "    x_train = tfio.IODataset.from_hdf5(train_file, dataset='/X_minmax')\n",
    "    y_train = tfio.IODataset.from_hdf5(train_file, dataset='/Y_log_scaled')\n",
    "    full_train = Dataset.zip((x_train, y_train))\n",
    "\n",
    "    # Load test data\n",
    "    with h5py.File(test_file, 'r') as h5:\n",
    "        num_test_samples, num_inputs,  = h5['X_minmax'].shape\n",
    "        _, num_flux,  = h5['Y_log_scaled'].shape\n",
    "    x_test = tfio.IODataset.from_hdf5(test_file, dataset='/X_minmax')\n",
    "    y_test = tfio.IODataset.from_hdf5(test_file, dataset='/Y_log_scaled')\n",
    "    test = Dataset.zip((x_test, y_test))\n",
    "\n",
    "    # Get number of training samples (from the dataset)\n",
    "    train_size = int(np.floor(num_train_samples * train_size_fraction))\n",
    "    print(f\"Number of training samples: {train_size} out of {num_train_samples} total\")\n",
    "    print(f\"Number of test samples: {num_test_samples}\")\n",
    "\n",
    "    # Choose seed based on model version\n",
    "    data_seeds = {\n",
    "        'd1': 42,\n",
    "        'd2': 87,\n",
    "        'd3': 5,\n",
    "        'd4': 98,\n",
    "        'd5': 123,\n",
    "    }\n",
    "    data_seed = data_seeds.get(data_version, None)\n",
    "\n",
    "    if bootstrap == 'b1':\n",
    "        # Reproducible bootstrap indices\n",
    "        rng = np.random.default_rng(data_seed)\n",
    "        sampled_indices = rng.integers(low=0, high=num_train_samples, size=train_size)\n",
    "\n",
    "        # Load dataset into memory\n",
    "        train_list = list(full_train.as_numpy_iterator())\n",
    "\n",
    "        # Sample with replacement\n",
    "        bootstrapped_data = [train_list[i] for i in sampled_indices]\n",
    "\n",
    "        # Separate into inputs and outputs\n",
    "        x_bootstrap, y_bootstrap = zip(*bootstrapped_data)\n",
    "\n",
    "        # Convert back to tf.data.Dataset\n",
    "        train = Dataset.from_tensor_slices((list(x_bootstrap), list(y_bootstrap)))\n",
    "\n",
    "    else:\n",
    "        # Shuffle deterministically\n",
    "        if data_version in data_seeds:\n",
    "            train_shuffled = full_train.shuffle(\n",
    "                buffer_size=num_train_samples, seed=data_seed, reshuffle_each_iteration=False\n",
    "            )\n",
    "        else:\n",
    "            train_shuffled = full_train\n",
    "\n",
    "        # Take subset without replacement\n",
    "        train = train_shuffled.take(train_size)\n",
    "\n",
    "    # Set batch_size to 128 unless the train size is smaller than 128, then set it to the train size.\n",
    "    if train_size < 128:\n",
    "        batch_size = train_size\n",
    "    else:\n",
    "        batch_size = 128\n",
    "\n",
    "    train = train.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "    test = test.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train, test, train_size, num_test_samples, batch_size, num_inputs\n",
    "\n",
    "def build_model(input_dim, n_layers, units, embedding_method, embed_dim=12, n_bins=48, n_frequencies=8, value_range=None):\n",
    "    print(f\"Building model with embedding {embedding_method}, {n_layers} layers, {units} units per layer, embed_dim {embed_dim}, n_bins {n_bins}, n_frequencies {n_frequencies}, value_range {value_range}\")\n",
    "\n",
    "    model = keras.Sequential([keras.Input(shape=(input_dim,), dtype=\"float32\")])\n",
    "\n",
    "    # Tabular embedding layer\n",
    "    if embedding_method == \"periodic\":\n",
    "        # Defaults: k=64, sigma=0.02, activation=True (you can change)\n",
    "        model.add(PeriodicEmbeddings(\n",
    "            n_features=input_dim,\n",
    "            n_frequencies=n_frequencies,\n",
    "            learnable_frequencies=True,\n",
    "            use_phase=True,\n",
    "            learnable_phases=True,\n",
    "        ))\n",
    "        model.add(keras.layers.Flatten())\n",
    "    elif embedding_method == \"piecewise_encoding\":\n",
    "        if value_range is None:\n",
    "            model.add(PiecewiseLinearEncoding(\n",
    "                n_features=input_dim,\n",
    "                n_bins=n_bins,\n",
    "                use_adaptive_range=True,\n",
    "                clip=True,\n",
    "            ))\n",
    "        else:\n",
    "            model.add(PiecewiseLinearEncoding(\n",
    "                n_features=input_dim,\n",
    "                n_bins=n_bins,\n",
    "                use_adaptive_range=False,\n",
    "                value_range=value_range,\n",
    "                clip=True,\n",
    "            ))\n",
    "    elif embedding_method == \"piecewise_embedding\":\n",
    "        if value_range is None:\n",
    "            model.add(PiecewiseLinearEmbeddings(\n",
    "                n_features=input_dim,\n",
    "                n_bins=n_bins,\n",
    "                d_embedding=embed_dim,\n",
    "                activation=True,          # ReLU(Linear(PLE))\n",
    "                use_adaptive_range=True,\n",
    "                clip=True,\n",
    "            ))\n",
    "        else:\n",
    "            model.add(PiecewiseLinearEmbeddings(\n",
    "                n_features=input_dim,\n",
    "                n_bins=n_bins,\n",
    "                d_embedding=embed_dim,\n",
    "                activation=True,          # ReLU(Linear(PLE))\n",
    "                use_adaptive_range=False,\n",
    "                value_range=value_range,\n",
    "                clip=True,\n",
    "            ))\n",
    "        model.add(keras.layers.Flatten())\n",
    "    else:\n",
    "        # No embedding, use raw inputs\n",
    "        pass\n",
    "\n",
    "    # If you’re using SELU, pair with lecun_normal + AlphaDropout (recommended for SELU)\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "        # optional:\n",
    "        # model.add(keras.layers.AlphaDropout(0.05))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e39df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:35:19.105579: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-10-07 16:35:19.110596: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2025-10-07 16:35:19.699336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9614 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 17888 out of 1788892 total\n",
      "Number of test samples: 198766\n",
      "Building model with embedding piecewise_embedding, 3 layers, 2048 units per layer, embed_dim 16, n_bins 8, n_frequencies 8, value_range (array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ piecewise_linear_embeddings     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,169</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PiecewiseLinearEmbeddings</span>)     │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │    <span style=\"color: #00af00; text-decoration-color: #00af00\">264,192</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,196,352</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,196,352</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,568</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ piecewise_linear_embeddings     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │      \u001b[38;5;34m1,169\u001b[0m │\n",
       "│ (\u001b[38;5;33mPiecewiseLinearEmbeddings\u001b[0m)     │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │    \u001b[38;5;34m264,192\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │  \u001b[38;5;34m4,196,352\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │  \u001b[38;5;34m4,196,352\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │     \u001b[38;5;34m65,568\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,723,633</span> (33.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,723,633\u001b[0m (33.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,723,616</span> (33.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,723,616\u001b[0m (33.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> (68.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17\u001b[0m (68.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:35:38.586984: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 284673 of 1788892\n",
      "2025-10-07 16:35:53.952978: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 288769 of 1788892\n",
      "2025-10-07 16:36:10.264702: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 357377 of 1788892\n",
      "2025-10-07 16:36:22.571705: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 364545 of 1788892\n",
      "2025-10-07 16:36:38.313476: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 397313 of 1788892\n",
      "2025-10-07 16:36:51.802647: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 399361 of 1788892\n",
      "2025-10-07 16:37:07.393890: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 401409 of 1788892\n",
      "2025-10-07 16:37:20.366119: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 405505 of 1788892\n",
      "2025-10-07 16:37:43.603731: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 408577 of 1788892\n",
      "2025-10-07 16:37:59.815276: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 410625 of 1788892\n",
      "2025-10-07 16:38:11.273768: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 412673 of 1788892\n",
      "2025-10-07 16:38:28.160590: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 417793 of 1788892\n",
      "2025-10-07 16:38:49.311778: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1466369 of 1788892\n",
      "2025-10-07 16:38:59.508818: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1471489 of 1788892\n",
      "2025-10-07 16:39:24.473108: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1544193 of 1788892\n",
      "2025-10-07 16:39:39.254322: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1547265 of 1788892\n",
      "2025-10-07 16:40:06.047652: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1561601 of 1788892\n",
      "2025-10-07 16:40:21.159390: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1563649 of 1788892\n",
      "2025-10-07 16:40:34.248705: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1565697 of 1788892\n",
      "2025-10-07 16:40:52.905014: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1568769 of 1788892\n",
      "2025-10-07 16:41:09.830117: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1570817 of 1788892\n",
      "2025-10-07 16:41:21.022333: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1572865 of 1788892\n",
      "2025-10-07 16:41:40.582197: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1674241 of 1788892\n",
      "2025-10-07 16:41:59.544687: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1677313 of 1788892\n",
      "2025-10-07 16:42:10.445834: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1679361 of 1788892\n",
      "2025-10-07 16:42:31.739609: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1683457 of 1788892\n",
      "2025-10-07 16:42:51.829232: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1692673 of 1788892\n",
      "2025-10-07 16:43:01.949637: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1694721 of 1788892\n",
      "2025-10-07 16:43:18.527437: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1697793 of 1788892\n",
      "2025-10-07 16:43:33.592186: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1699841 of 1788892\n",
      "2025-10-07 16:43:44.333203: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n",
      "2025-10-07 16:43:44.354477: I external/local_xla/xla/service/service.cc:168] XLA service 0x1521d0013590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-07 16:43:44.354501: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-10-07 16:43:44.727039: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-07 16:43:47.155284: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759891428.581260  382472 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1759891428.610183  382472 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "2025-10-07 16:43:49.474896: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6033276781302955739\n",
      "2025-10-07 16:43:49.474947: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8127168858024198982\n",
      "/home/linneamw/anaconda3/envs/gcr-gpu/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "W0000 00:00:1759891434.899235  382472 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "2025-10-07 16:43:57.557817: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6033276781302955739\n",
      "2025-10-07 16:43:57.557854: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8127168858024198982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 - 514s - 4s/step - loss: 0.2186 - mse: 0.4780 - val_loss: 0.0389 - val_mse: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:45:04.674622: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 110593 of 1788892\n",
      "2025-10-07 16:45:15.917097: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 114689 of 1788892\n",
      "2025-10-07 16:45:32.288268: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 117761 of 1788892\n",
      "2025-10-07 16:45:57.030696: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 132097 of 1788892\n",
      "2025-10-07 16:46:10.757010: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 134145 of 1788892\n",
      "2025-10-07 16:46:21.593224: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 135169 of 1788892\n",
      "2025-10-07 16:46:32.037786: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 137217 of 1788892\n",
      "2025-10-07 16:46:42.609787: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 175105 of 1788892\n",
      "2025-10-07 16:47:02.353531: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 190465 of 1788892\n",
      "2025-10-07 16:47:20.632502: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 507905 of 1788892\n",
      "2025-10-07 16:47:30.657142: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 722945 of 1788892\n",
      "2025-10-07 16:47:50.618639: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1131521 of 1788892\n",
      "2025-10-07 16:48:00.626390: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1147905 of 1788892\n",
      "2025-10-07 16:48:11.147046: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 1465345 of 1788892\n",
      "2025-10-07 16:48:21.673888: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n",
      "2025-10-07 16:48:22.390343: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6033276781302955739\n",
      "2025-10-07 16:48:22.390402: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8127168858024198982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 - 264s - 2s/step - loss: 0.0316 - mse: 0.0016 - val_loss: 0.0247 - val_mse: 9.6294e-04 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:48:24.870583: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6033276781302955739\n",
      "2025-10-07 16:48:24.870618: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8127168858024198982\n",
      "2025-10-07 16:49:17.492152: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6033276781302955739\n",
      "2025-10-07 16:49:17.492203: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8127168858024198982\n",
      "2025-10-07 16:49:20.464015: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6033276781302955739\n",
      "2025-10-07 16:49:20.464051: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8127168858024198982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.024613454937934875, Train MSE: 0.000958481163252145\n",
      "Test MAE: 0.024706387892365456, Test MSE: 0.000962943013291806\n"
     ]
    }
   ],
   "source": [
    "# Fixed args – customize if needed\n",
    "args = {\n",
    "    \"polarity\": \"neg\",\n",
    "    \"train_size_fraction\": 0.01,\n",
    "    \"bootstrap\": \"b0\",\n",
    "    \"data_version\": \"d1\"\n",
    "}\n",
    "\n",
    "# Sample hyperparameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "train, test, train_size, num_test_samples, batch_size, num_inputs = load_dataset(\n",
    "    args[\"polarity\"], args[\"data_version\"], args[\"train_size_fraction\"], args[\"bootstrap\"]\n",
    ")\n",
    "\n",
    "# For piecewise linear embeddings, we need to provide the value range (min, max) for each feature.\n",
    "# All input data is min-max scaled already\n",
    "n_features = 8\n",
    "mins = np.zeros(n_features, dtype=\"float32\")\n",
    "maxs = np.ones(n_features, dtype=\"float32\")\n",
    "value_range = (mins, maxs)\n",
    "\n",
    "# Get trial hyperparameters\n",
    "n_layers = 3\n",
    "units = 2048\n",
    "embedding_method = \"piecewise_embedding\"\n",
    "n_frequencies = 8\n",
    "n_bins = 8\n",
    "embed_dim = 16\n",
    "\n",
    "# Define model\n",
    "model = build_model(\n",
    "    num_inputs, \n",
    "    n_layers, \n",
    "    units, \n",
    "    embedding_method,\n",
    "    embed_dim=embed_dim, \n",
    "    n_frequencies=n_frequencies, \n",
    "    n_bins=n_bins, \n",
    "    value_range=value_range\n",
    ")\n",
    "print(model.summary())\n",
    "\n",
    "# Compile model\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mse'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=2,\n",
    "    validation_data=test,\n",
    "    shuffle=False,\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate on train\n",
    "train_results = model.evaluate(train, verbose=0)\n",
    "train_mae = train_results[0]  # This is the loss (MAE)\n",
    "train_mse = train_results[1]  # This is the metric (MSE)\n",
    "\n",
    "# Evaluate on test\n",
    "test_results = model.evaluate(test, verbose=0)\n",
    "test_mae = test_results[0]\n",
    "test_mse = test_results[1]\n",
    "\n",
    "print(f\"Train MAE: {train_mae}, Train MSE: {train_mse}\")\n",
    "print(f\"Test MAE: {test_mae}, Test MSE: {test_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
