{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import keras_core as keras\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, vmap, jit, grad\n",
    "#assert jax.default_backend() == 'gpu'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#import elegy # pip install elegy. # Trying to do this with keras core instead.\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import preprocess.preprocess\n",
    "from preprocess.preprocess import INPUTS, PARAMETERS, PARAMETERS_SPECIFIED, RIGIDITY_VALS\n",
    "from preprocess.preprocess import transform_input, untransform_input\n",
    "from preprocess.preprocess import PARAMETERS_MIN, PARAMETERS_MAX\n",
    "from chi2 import CalculateChi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check new yearly data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need version with correct path to the data\n",
    "def index_mcmc_runs(file_version):\n",
    "    \"\"\"Make a list of combinations for which we want to run MCMC.\"\"\"\n",
    "    if file_version == '2023':\n",
    "        experiments = ['AMS02_H-PRL2021', 'PAMELA_H-ApJ2013', 'PAMELA_H-ApJL2018']\n",
    "        dfs = []\n",
    "        for experiment_name in experiments:\n",
    "            filename = f'../../data/2023/{experiment_name}_heliosphere.dat'\n",
    "            df = utils.index_experiment_files(filename) \n",
    "            df['experiment_name'] = experiment_name\n",
    "            df['filename_heliosphere'] = filename\n",
    "            dfs.append(df)\n",
    "        df = pd.concat(dfs, axis=0, ignore_index=0)\n",
    "\n",
    "    elif file_version == '2024':\n",
    "        filename = f'../../data/2024/yearly_heliosphere.dat'\n",
    "        df = utils.read_experiment_summary(filename)\n",
    "        df['experiment_name'] = 'yearly'\n",
    "        df['filename_heliosphere'] = filename\n",
    "\n",
    "    else: raise ValueError(f\"Unknown file_version {file_version}. Must be '2023' or '2024'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 combinations to run MCMC on. Performing MCMC on index 0.\n"
     ]
    }
   ],
   "source": [
    "SLURM_ARRAY_TASK_ID = 0\n",
    "SLURM_ARRAY_JOB_ID = 0\n",
    "DEBUG = True\n",
    "\n",
    "# Version specifications\n",
    "model_version = 'v3.0' # v2.0 is MSE NN, v3.0 is MAE NN\n",
    "hmc_version = 'v25.0'\n",
    "file_version = '2024'\n",
    "\n",
    "# Select experiment parameters\n",
    "df = index_mcmc_runs(file_version=file_version)  # List of all experiments (0-209) for '2023', 0-14 for '2024'\n",
    "print(f'Found {df.shape[0]} combinations to run MCMC on. Performing MCMC on index {SLURM_ARRAY_TASK_ID}.')\n",
    "df = df.iloc[SLURM_ARRAY_TASK_ID]\n",
    "\n",
    "# Define parameters for HMC\n",
    "seed = SLURM_ARRAY_TASK_ID + SLURM_ARRAY_JOB_ID\n",
    "penalty = 1e6\n",
    "integrate = False # If False, Chi2 is interpolated. If True, Chi2 is integrated.\n",
    "par_equals_perr = False # If True, only 3 parameters will be sampled by the HMC and pwr1par==pwr1perr and pwr2par==pwr2perr\n",
    "constant_vspoles = False # If True, vspoles is fixed to 400.0. If False, vspoles is specified in the data file.\n",
    "specified_parameters = utils.get_parameters(df.filename_heliosphere, df.interval, constant_vspoles=constant_vspoles)\n",
    "\n",
    "# Number of parameters for HMC to sample. 5 normally, 3 if par_equals_perr=True\n",
    "if par_equals_perr:\n",
    "    num_params = 3\n",
    "else:\n",
    "    num_params = 5\n",
    "\n",
    "# Load observation data and define logprob. \n",
    "if file_version == '2023': \n",
    "    data_path = f'../../data/oct2022/{df.experiment_name}/{df.experiment_name}_{df.interval}.dat'  # This data is the same.\n",
    "elif file_version == '2024': \n",
    "    year = 2000 + SLURM_ARRAY_TASK_ID # assumes only negative intervals. If otherwise, fix this\n",
    "    data_path = f'../../data/2024/yearly/{year}.dat'\n",
    "else:\n",
    "    raise ValueError(f\"Invalid file_version {file_version}. Must be '2023' or '2024'.\")\n",
    "\n",
    "model_path = f'../../models/model_{model_version}_{df.polarity}.keras'\n",
    "\n",
    "# Define log probability\n",
    "target_log_prob = utils.define_log_prob(model_path, data_path, specified_parameters, penalty=penalty, integrate=integrate, par_equals_perr=par_equals_perr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bins (15): [0.28 0.76 0.83 0.9  0.98 1.06 1.16 1.26 1.38 1.51 1.66 1.83 2.02 2.23\n",
      " 2.47]\n",
      "Bin midpoints (15): [0.29933259 0.79422919 0.86429162 0.93914855 1.01921538 1.1088733\n",
      " 1.2089665  1.31863566 1.44353732 1.58322456 1.74292857 1.92265442\n",
      " 2.1224043  2.34693417]\n",
      "Observed: [ 30.39873 381.5869  378.8598  423.6459  382.2285  427.3153  394.8742\n",
      " 407.5119  394.1613  395.7958  413.7841  395.2794  356.6092  297.1244 ]\n"
     ]
    }
   ],
   "source": [
    "bins, bin_midpoints, observed, uncertainty = utils.load_data_ams(data_path, integrate)\n",
    "\n",
    "print(f'Bins ({len(bins)}): {bins}')\n",
    "print(f'Bin midpoints ({len(bins)}): {bin_midpoints}')\n",
    "print(f'Observed: {observed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that calculated chi2 matches results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max logprob: -1.5840998888015747 at index 71052\n"
     ]
    }
   ],
   "source": [
    "results_dir = '../../../results/' + hmc_version + '/'\n",
    "\n",
    "# Load results_dir/logprobs_{SLURM_ARRAY_TASK_ID}_yearly_{year}.csv, and find the absolute value max logprob value and it's index\n",
    "logprobs = pd.read_csv(results_dir + f'logprobs_{SLURM_ARRAY_TASK_ID}_yearly_{year}_neg.csv', header=None)\n",
    "max_logprob = logprobs.max()\n",
    "max_logprob_idx = logprobs.idxmax()\n",
    "\n",
    "print(f'Max logprob: {max_logprob[0]} at index {max_logprob_idx[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max logprob sample: 0    431.610474\n",
      "1      1.533347\n",
      "2      1.112190\n",
      "3      0.408744\n",
      "4      0.431167\n",
      "Name: 71052, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get samples from results_dir/samples_{SLURM_ARRAY_TASK_ID}_yearly_{year}_neg.csv\n",
    "samples = pd.read_csv(results_dir + f'samples_{SLURM_ARRAY_TASK_ID}_yearly_{year}_neg.csv', header=None)\n",
    "\n",
    "# Get the sample at the max logprob index\n",
    "max_logprob_sample = samples.iloc[max_logprob_idx[0]]\n",
    "\n",
    "print(f'Max logprob sample: {max_logprob_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs: [4.31610474e+02 1.53334725e+00 1.11219001e+00 4.08743829e-01\n",
      " 4.31167215e-01]\n",
      "Transformed xs: [0.43066295 0.87180558 0.54783847 0.00460202 0.0164038 ]\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "for i in range(5):\n",
    "    xs.append(max_logprob_sample[i])\n",
    "\n",
    "xs = np.array(xs)\n",
    "\n",
    "print(f'xs: {xs}')\n",
    "\n",
    "# Transform xs\n",
    "xs = transform_input(xs)\n",
    "\n",
    "print(f'Transformed xs: {xs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break apart define_log_prob function and check each piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specified_parameters: (68.69, 7.03, 538.4)\n",
      "parameters_specified_transformed: [0.8081177 0.6471429 0.4613334]\n"
     ]
    }
   ],
   "source": [
    "# Load trained NN model that maps 8 parameters to predicted flux at RIGIDITY_VALS.\n",
    "model = keras.models.load_model(model_path)\n",
    "model.run_eagerly = True # Settable attribute (in elegy). Required to be true for ppmodel.\n",
    "\n",
    "# Load observation data from Claudio\n",
    "if integrate:\n",
    "    bins, r1r2, observed, uncertainty = utils.load_data_ams(data_path, integrate)\n",
    "else:\n",
    "    bins, bin_midpoints, observed, uncertainty = utils.load_data_ams(data_path, integrate)\n",
    "\n",
    "# Transform input parameters to be in range 0--1.\n",
    "parameters_specified_transformed = transform_input(jnp.array(specified_parameters))\n",
    "print(f'specified_parameters: {specified_parameters}')\n",
    "print(f'parameters_specified_transformed: {parameters_specified_transformed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max logprob Chi2: 3.1681997776031494\n"
     ]
    }
   ],
   "source": [
    "# If par==perr, then only predicting ['cpa', 'pwr1par', 'pwr2par']. Need to create array of ['cpa', 'pwr1par', 'pwr1par', 'pwr2par', 'pwr2par']\n",
    "if par_equals_perr:\n",
    "    xs = jnp.array([xs[0], xs[1], xs[1], xs[2], xs[2]])\n",
    "\n",
    "# Include logprior in loglikelihood. This keeps HMC from going off into no-mans land.\n",
    "nlogprior = 0.\n",
    "for i in range(5):\n",
    "    nlogprior += penalty * jnp.abs((jnp.minimum(0., xs[i]))) # Penalty for being <0\n",
    "    nlogprior += penalty * jnp.abs((jnp.maximum(1., xs[i]) - 1.))  # Penalty for being >1\n",
    "\n",
    "# log_prob = -chi2/2.  - nlogprior, so using nlogprior find the chi2 value associated with the max logprob\n",
    "chi2_max_logprob = -2. * (max_logprob[0] + nlogprior)\n",
    "print(f'Max logprob Chi2: {chi2_max_logprob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch parameters and predict with model\n",
    "batch = utils._form_batch(xs, parameters_specified_transformed)\n",
    "yhat = model(batch)    \n",
    "yhat = yhat[0,:]  # Remove batch dimension.\n",
    "yhat = yhat.numpy()\n",
    "\n",
    "# Prepare yhat and rigidity for integration/interpolation\n",
    "yhat = utils.untransform_output(yhat.reshape((1,-1))).reshape(-1) # Undo scaling and minmax.\n",
    "log_yhat = jnp.log(yhat)\n",
    "log_rigidity = jnp.log(jnp.array(RIGIDITY_VALS))\n",
    "\n",
    "# Compute chi2\n",
    "chi2 = CalculateChi2(log_rigidity, log_yhat)\n",
    "yhat_interp_integrated = []\n",
    "\n",
    "if integrate:\n",
    "    # Compute integral for each r1-r2 range\n",
    "    for x1, x2 in r1r2:\n",
    "        integral = chi2.compute_integral(x1, x2) / (x2 - x1)\n",
    "        yhat_interp_integrated.append(integral)\n",
    "else:\n",
    "    # Compute interpolated value at each bin_midpoint\n",
    "    for x in bin_midpoints:\n",
    "        interpolated = chi2.interpolate_model(x)\n",
    "        yhat_interp_integrated.append(interpolated)\n",
    "\n",
    "# Compute log prob\n",
    "chi2 = (((jnp.asarray(yhat_interp_integrated) - observed)/uncertainty)**2)\n",
    "cum_chi2 = jnp.sum(chi2)\n",
    "log_prob = -cum_chi2/2.  - nlogprior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max logprob model index (0-based): 71052; logprob = -1.584, chi2 = 3.168; cpa = 431.61, pwr1par = 1.53, pwr1perr = 1.11, pwr2par = 0.41, pwr2perr = 0.43\n",
      "Calculated chi2 and logprob from sample parameters: logprob = -1.584; chi2 = 3.168\n",
      "\n",
      "rig = 0.299333 [0.28, 0.76]; data = 30.398730 ; unc = 0.352263; mod: 30.417137 ; cum_chi2 = 0.002731\n",
      "rig = 0.794229 [0.76, 0.83]; data = 381.586900 ; unc = 76.412730; mod: 293.324677 ; cum_chi2 = 1.336923\n",
      "rig = 0.864292 [0.83, 0.90]; data = 378.859800 ; unc = 75.866600; mod: 328.203674 ; cum_chi2 = 1.782746\n",
      "rig = 0.939149 [0.90, 0.98]; data = 423.645900 ; unc = 84.835040; mod: 359.885223 ; cum_chi2 = 2.347625\n",
      "rig = 1.019215 [0.98, 1.06]; data = 382.228500 ; unc = 76.541200; mod: 392.363617 ; cum_chi2 = 2.365159\n",
      "rig = 1.108873 [1.06, 1.16]; data = 427.315300 ; unc = 85.569810; mod: 408.579590 ; cum_chi2 = 2.413099\n",
      "rig = 1.208967 [1.16, 1.26]; data = 394.874200 ; unc = 79.073500; mod: 425.897736 ; cum_chi2 = 2.567028\n",
      "rig = 1.318636 [1.26, 1.38]; data = 407.511900 ; unc = 81.604190; mod: 434.162201 ; cum_chi2 = 2.673683\n",
      "rig = 1.443537 [1.38, 1.51]; data = 394.161300 ; unc = 78.930740; mod: 429.485809 ; cum_chi2 = 2.873973\n",
      "rig = 1.583225 [1.51, 1.66]; data = 395.795800 ; unc = 79.258040; mod: 424.764587 ; cum_chi2 = 3.007563\n",
      "rig = 1.742929 [1.66, 1.83]; data = 413.784100 ; unc = 82.860210; mod: 400.537109 ; cum_chi2 = 3.033122\n",
      "rig = 1.922654 [1.83, 2.02]; data = 395.279400 ; unc = 79.154630; mod: 373.198883 ; cum_chi2 = 3.110937\n",
      "rig = 2.122404 [2.02, 2.23]; data = 356.609200 ; unc = 71.410920; mod: 343.631897 ; cum_chi2 = 3.143962\n",
      "rig = 2.346934 [2.23, 2.47]; data = 297.124400 ; unc = 59.499110; mod: 306.388824 ; cum_chi2 = 3.168207\n"
     ]
    }
   ],
   "source": [
    "xs_untransformed = untransform_input(xs)\n",
    "print(f\"Max logprob model index (0-based): {max_logprob_idx[0]}; logprob = {max_logprob[0]:.3f}, chi2 = {chi2_max_logprob:.3f}; cpa = {xs_untransformed[0]:.2f}, pwr1par = {xs_untransformed[1]:.2f}, pwr1perr = {xs_untransformed[2]:.2f}, pwr2par = {xs_untransformed[3]:.2f}, pwr2perr = {xs_untransformed[4]:.2f}\")\n",
    "print(f\"Calculated chi2 and logprob from sample parameters: logprob = {log_prob:.3f}; chi2 = {cum_chi2:.3f}\\n\")\n",
    "\n",
    "for i in range(len(yhat_interp_integrated)):\n",
    "    print(f'rig = {bin_midpoints[i]:.6f} [{bins[i]:.2f}, {bins[i+1]:.2f}]; data = {observed[i]:.6f}',\n",
    "          f'; unc = {uncertainty[i]:.6f}; mod: {yhat_interp_integrated[i]:.6f}',\n",
    "          f'; cum_chi2 = {(chi2[:i+1].sum()):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check older data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133 combinations to run MCMC on. Performing MCMC on index 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "interval     20110520-20110610\n",
       "alpha                    51.49\n",
       "cmf                       4.85\n",
       "vspoles                 632.52\n",
       "alpha_std                10.69\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLURM_ARRAY_TASK_ID = 0\n",
    "SLURM_ARRAY_JOB_ID = 0\n",
    "DEBUG = True\n",
    "\n",
    "# Version specifications\n",
    "model_version = 'v3.0' # v2.0 is MSE NN, v3.0 is MAE NN\n",
    "hmc_version = 'v24.3'\n",
    "file_version = '2023'\n",
    "\n",
    "# Select experiment parameters\n",
    "df = index_mcmc_runs(file_version=file_version)  # List of all experiments (0-209) for '2023', 0-14 for '2024'\n",
    "print(f'Found {df.shape[0]} combinations to run MCMC on. Performing MCMC on index {SLURM_ARRAY_TASK_ID}.')\n",
    "df = df.iloc[SLURM_ARRAY_TASK_ID]\n",
    "\n",
    "# Load observation data and define logprob. \n",
    "if file_version == '2023': \n",
    "    data_path = f'../../data/oct2022/{df.experiment_name}/{df.experiment_name}_{df.interval}.dat'  # This data is the same.\n",
    "elif file_version == '2024': \n",
    "    year = 2000 + SLURM_ARRAY_TASK_ID # assumes only negative intervals. If otherwise, fix this\n",
    "    data_path = f'../../data/2024/yearly/{year}.dat'\n",
    "else:\n",
    "    raise ValueError(f\"Invalid file_version {file_version}. Must be '2023' or '2024'.\")\n",
    "\n",
    "model_path = f'../../models/model_{model_version}_{df.polarity}.keras'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from ../../data/oct2022/AMS02_H-PRL2021/AMS02_H-PRL2021_20110520-20110610.dat.\n",
      "Bins: [  1.     1.16   1.33   1.51   1.71   1.92   2.15   2.4    2.67   2.97\n",
      "   3.29   3.64   4.02   4.43   4.88   5.37   5.9    6.47   7.09   7.76\n",
      "   8.48   9.26  10.1   11.    13.    16.6   22.8   33.5   48.5   69.7\n",
      " 100.  ]\n",
      "Observed: [9.542576e+02 9.411921e+02 8.769211e+02 8.003528e+02 7.088688e+02\n",
      " 6.185521e+02 5.311145e+02 4.512981e+02 3.792883e+02 3.166651e+02\n",
      " 2.633877e+02 2.184992e+02 1.800327e+02 1.470573e+02 1.199892e+02\n",
      " 9.722954e+01 7.856188e+01 6.343624e+01 5.116549e+01 4.125726e+01\n",
      " 3.316556e+01 2.672519e+01 2.151074e+01 1.559146e+01 9.113706e+00\n",
      " 4.317174e+00 1.667450e+00 5.843280e-01 2.086225e-01 7.601720e-02]\n",
      "Uncertainty: [2.811600e+01 2.148905e+01 1.643800e+01 1.293201e+01 1.028839e+01\n",
      " 8.293686e+00 6.719937e+00 5.448942e+00 4.419263e+00 3.583056e+00\n",
      " 2.932180e+00 2.403835e+00 1.970323e+00 1.589138e+00 1.290966e+00\n",
      " 1.053403e+00 8.565820e-01 6.939785e-01 5.606656e-01 4.578981e-01\n",
      " 3.716021e-01 2.988398e-01 2.424221e-01 1.775248e-01 1.064052e-01\n",
      " 5.168566e-02 2.057289e-02 7.323262e-03 2.647845e-03 1.011767e-03]\n"
     ]
    }
   ],
   "source": [
    "bins, bin_midpoints, observed, uncertainty = utils.load_data_ams(data_path, integrate)\n",
    "print(f\"Loaded dataset from {data_path}.\")\n",
    "print(f'Bins: {bins}')\n",
    "print(f'Observed: {observed}')\n",
    "print(f'Uncertainty: {uncertainty}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
